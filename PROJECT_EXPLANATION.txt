KARACHI AIR QUALITY INDEX (AQI) PREDICTION SYSTEM
==================================================


OVERVIEW
--------
This project is a complete air quality prediction system that automatically collects weather and pollution data, processes it into features, trains multiple machine learning models, and provides predictions through a web interface. It's designed to run continuously with automated updates every hour and daily model retraining.

PROJECT STRUCTURE
-----------------

1. DATA COLLECTION (Data_Collection folder)
   - data_fetch.py: This is the main data collection script that runs every hour
   - It connects to OpenMeteo API to get air quality data (PM2.5, PM10, NO2, SO2, CO, O3) and weather data (temperature, humidity, precipitation)
   - The script fetches data for Karachi (coordinates: 24.8607, 67.0011) and processes it into daily averages
   - It saves data in two formats: CSV for easy reading and Parquet for efficient storage
   - The script is smart - it only fetches new data and doesn't overwrite existing historical data

2. FEATURE STORE (feature_repo folder)
   - This is where your data structure is defined using Feast (a feature store system)
   - karachi_features_new.py defines what columns your data should have
   - It creates features like daily averages of air pollutants, weather conditions, and calculated features
   - The feature store helps organize your data and makes it easy for models to access the right information

3. DATA PROCESSING (Data folder)
   - feature_store: Contains the processed data files (CSV and Parquet)
   - processed: Contains intermediate processed data
   - raw: Contains the original collected data before processing

4. MACHINE LEARNING MODELS (Models folder)
   - train_lightgbm.py: Trains LightGBM models (a type of gradient boosting) for predicting AQI 1, 2, and 3 days ahead
   - train_hgbr.py: Trains Histogram Gradient Boosting models (another type of boosting algorithm)
   - train_linear.py: Trains linear regression models (simpler but sometimes effective)
   - train_rf.py: Trains Random Forest models (good for handling different types of data)
   - stacking_linear_lightgbm.py: Combines all the individual models into one ensemble model for better predictions
   - predict_realtime.py: Generates real-time predictions using the trained models
   - predict_latest.py: Generates predictions for the latest available data

5. MODEL REGISTRY (Models/Models/registry folder)
   - This is where all your trained models are stored
   - It keeps track of different versions of models, their performance metrics, and predictions
   - Each model type (LightGBM, HGBR, Linear, Random Forest) has its own files
   - The ensemble model weights are also stored here

6. EXPLORATORY DATA ANALYSIS (Models/EDA folder)
   - run_eda.py: Analyzes your data and model performance
   - Creates visualizations and reports showing how well each model performs
   - Generates feature importance rankings (which factors most affect air quality)
   - Creates summary files for each model type showing accuracy metrics

7. WEB APPLICATION (WebApp folder)
   - Backend (WebApp/Backend/app): The server that handles API requests and runs your models
     * main.py: The main server that coordinates everything
     * model_loader.py: Loads your trained models and makes predictions
     * feast_client.py: Connects to your feature store to get the latest data
   - Frontend (WebApp/Frontend): The user interface that people see in their web browser
     * gradio_app.py: Creates a nice web interface showing AQI predictions, historical data, and graphs

8. AUTOMATION (GitHub Workflows)
   - features-hourly.yml: Runs every hour to collect new data and update predictions
   - train-daily.yml: Runs every day at 2:15 AM to retrain all models with the latest data

HOW THE SYSTEM WORKS
--------------------

1. DATA COLLECTION PHASE (Every Hour)
   - The system automatically fetches the latest air quality and weather data for Karachi
   - It processes this data into daily features (averages, calculations, time-based features)
   - The data is saved to both CSV and Parquet files
   - Feast feature store is updated with the new data

2. FEATURE ENGINEERING PHASE
   - The system creates additional features like:
     * Time-based features (month, season, day of week)
     * Lag features (previous day's AQI values)
     * Rolling averages (3-day moving averages)
     * Logarithmic transformations of pollutant values
   - These features help the models understand patterns in the data

3. MODEL TRAINING PHASE (Daily)
   - Each model type (LightGBM, HGBR, Linear, Random Forest) is trained separately
   - The system uses the last 90 days of data for testing (holdout period)
   - Each model learns to predict AQI for 1 day ahead, 2 days ahead, and 3 days ahead
   - Performance metrics are calculated (RMSE, MAE, R-squared)

4. ENSEMBLE CREATION PHASE
   - The individual model predictions are combined using optimized weights
   - The system finds the best combination of models for each prediction horizon
   - This creates a more robust and accurate final prediction

5. PREDICTION GENERATION PHASE
   - Real-time predictions are generated using the latest available data
   - The system provides AQI predictions for the next 3 days
   - Each prediction includes confidence levels and model contributions

6. WEB INTERFACE PHASE
   - Users can access the system through a web browser
   - They can see current AQI values, historical trends, and future predictions
   - The interface shows predictions from individual models and the ensemble
   - Users can trigger manual data updates if needed

My Journey
So i started doing this project kind of blindly as i had prior experience with classification projects so i underestimated this project but due to the lack fo experience when i once started doing this project especially since itw as my frist time doing regression absed project i struggled alot with accuracy
the major reason was that i was getting stuck in data extraction and feature engineering taking help from chatgpt was making it worse my problem was that initially i was using Openweather api for data extraction and was extracting 4.5 years of data and was extracting 150+ features from it and gpt kept misguiding me that more features + better then i added open meteo for weather and it got tiny bit better but still extremely worse
i kept wasting time on trying different models since all of them were giving me below 0.4 R^2 for all 3 days which was awful i tried SARIMA ,LSTM,RNN,GRU,XGboost,Ctaboost and the rest i kept which agve slightly better preformance 
then i foudn out i should use shap to do better data cleaning and remvoe features that were making my model worse
i removed all the unimportant features even then my accuracy increased by 20% at best
at that point i had given up 
but then the next day i redid my whole work i did proper research found some other peoples codes saw what they did and how they got much ebtter accuries thats when i understood that i was keeping too many features and it was actually hurting my performance
i moved on then to only using open meteo for data (daata was much cleaner + it had weather data) i extracted only 33 features and my accuracy became way better 
im especially proud of my ensemble model which gives quite good results


TECHNICAL COMPONENTS
--------------------

1. DATA SOURCES
   - OpenMeteo API for air quality and weather data
   - Historical data storage in CSV and Parquet formats
   - Feature store for organized data access

2. MACHINE LEARNING FRAMEWORKS
   - LightGBM for gradient boosting
   - Scikit-learn for traditional machine learning
   - Custom ensemble methods for model combination

3. WEB TECHNOLOGIES
   - FastAPI for the backend server
   - Gradio for the user interface
   - RESTful API for data access

4. AUTOMATION TOOLS
   - GitHub Actions for scheduled workflows
   - Python scripts for data processing
   - Feast for feature store management

5. DATA PROCESSING
   - Pandas for data manipulation
   - NumPy for numerical operations
   - Custom feature engineering pipelines

MODEL PERFORMANCE RESULTS
-------------------------

Based on the latest training results (August 19, 2025), here's how well each model performs:

1. ENSEMBLE MODEL (COMBINED) - BEST OVERALL PERFORMANCE
   - Day 1 (hd1): RMSE: 4.03, MAE: 3.18, R²: 0.88, 
   - Day 2 (hd2): RMSE: 9.32, MAE: 7.24, R²: 0.32, 
   - Day 3 (hd3): RMSE: 10.67, MAE: 8.78, R²: 0.07, 

2. INDIVIDUAL MODEL PERFORMANCE

   LIGHTGBM MODELS:
   - Day 1: RMSE: 4.25, MAE: 3.14, R²: 0.86, 
   - Day 2: RMSE: 9.98, MAE: 7.80, R²: 0.23, 
   - Day 3: RMSE: 12.74, MAE: 9.50, R²: -0.27, 

   RANDOM FOREST MODELS:
   - Day 1: RMSE: 4.12, MAE: 3.26, R²: 0.87, 
   - Day 2: RMSE: 9.36, MAE: 7.28, R²: 0.32,
   - Day 3: RMSE: 11.98, MAE: 8.92, R²: -0.13, 

   HISTOGRAM GRADIENT BOOSTING (HGBR):
   - Day 1: RMSE: 4.09, MAE: 3.15, R²: 0.87, 
   - Day 2: RMSE: 9.90, MAE: 7.48, R²: 0.24, 
   - Day 3: RMSE: 10.67, MAE: 8.78, R²: 0.07, 

   LINEAR MODELS:
   - Day 1: RMSE: 4.99, MAE: 4.01, R²: 0.81, 
   - Day 2: RMSE: 9.85, MAE: 7.78, R²: 0.25, 
   - Day 3: RMSE: 12.33, MAE: 9.82, R²: -0.19, 


5. MODEL WEIGHTS IN ENSEMBLE using Least Squares Optimization

   DAY 1 (hd1):
   - Random Forest: 39.8% (highest contribution)
   - HGBR: 32.0%
   - Linear: 24.3%
   - LightGBM: 3.9%

   DAY 2 (hd2):
   - Random Forest: 82.6% (dominant model)
   - Linear: 9.7%
   - HGBR: 7.7%
   - LightGBM: 0% (excluded)

   DAY 3 (hd3):
   - HGBR: 70.9% (dominant model)
   - LightGBM: 24.4%
   - Linear: 4.7%
   - Random Forest: 0% (excluded)





