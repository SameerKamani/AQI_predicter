DATA COLLECTION, FEATURE ENGINEERING, AND FEATURE STORE DOCUMENTATION
================================================================

PROJECT: Karachi Air Quality Index (AQI) Prediction System
DATE: August 15, 2025
VERSION: 1.0

================================================================
1. DATA COLLECTION PROCESS
================================================================

OVERVIEW:
We implemented a comprehensive data collection pipeline to gather air quality and meteorological data for Karachi, Pakistan, enabling accurate AQI predictions.

DATA SOURCES:
- OpenWeather Air Pollution API: Historical air quality data (PM2.5, PM10, CO, NO2, O3, SO2, NH3)
- Open-Meteo ERA5 Archive: Historical meteorological data (temperature, humidity, pressure, wind, precipitation)
- Location: Karachi (24.8607°N, 67.0011°E)

API SPECIFICATIONS:
- OpenWeather Air Pollution API: https://api.openweathermap.org/data/2.5/air_pollution/history
- Open-Meteo ERA5: https://archive-api.open-meteo.com/v1/era5
- Rate Limits: OpenWeather (1000 calls/day), Open-Meteo (10,000 calls/day)
- Authentication: API key required for OpenWeather, free access for Open-Meteo

COLLECTION PIPELINE:
1. Automated Data Fetching: Scripts fetch data for specified date ranges with configurable parameters
2. Rate Limiting: Built-in delays (1-2 seconds) between API calls to respect rate limits
3. Error Handling: Automatic retry logic for transient API failures (429, 500, 502, 503, 504)
4. Data Validation: Minimum hourly observations per day (configurable, default: 20) to ensure data quality
5. Gap Imputation: Optional interpolation for missing calendar days to maintain data continuity

EXECUTION:
- Primary Script: run_all.ps1 (PowerShell automation)
- Data Collection: feature_store_pipeline.py (Python pipeline)
- Standalone Fetching: fetch_openweather_karachi.py (individual data source)
- Default Range: Last 90 days (configurable up to 1095 days)

================================================================
2. FEATURE ENGINEERING
================================================================

FEATURE CATEGORIES:
Our dataset comprises 113 engineered features across multiple categories:

CORE AIR QUALITY FEATURES:
- Daily means: PM2.5, PM10, CO, NO, NO2, O3, SO2, NH3
- AQI calculations: EPA standard air quality index
- Ozone metrics: 8-hour maximum concentrations (o3_8h_max_ppb)

METEOROLOGICAL FEATURES:
- Temperature: mean, min, max, range
- Humidity: mean, dew point
- Pressure: atmospheric pressure
- Wind: speed (mean, max), gust, direction components (U, V)
- Precipitation: rain sum, snow sum
- Visibility and cloud cover

TEMPORAL FEATURES:
- Calendar: day of week, month, year, weekend indicators
- Cyclical: sine/cosine transformations of day-of-year
- Completeness: is_complete_day, imputed flags

LAGGED FEATURES:
- AQI lags: 1-7 day historical values
- Pollutant lags: PM2.5, PM10 lags for 1-7 days
- Weather lags: Temperature, humidity, wind components (1-3 day lags)
- Enables temporal dependency modeling

ROLLING STATISTICS:
- Moving averages: 3, 7, 14, 30-day windows
- Rolling standard deviations: capture volatility patterns
- Applied to AQI, PM2.5, PM10, and weather metrics (3, 7-day windows)

CHANGE RATE FEATURES:
- AQI change rate: day-over-day AQI changes

TARGET VARIABLES:
- target_aqi_d1: AQI prediction for next day
- target_aqi_d2: AQI prediction for 2 days ahead
- target_aqi_d3: AQI prediction for 3 days ahead

DATA QUALITY FLAGS:
- num_hours: Number of hourly observations per day
- is_complete_day: Flag indicating if day has sufficient data (≥16 hours)
- imputed: Flag indicating if data was interpolated for missing calendar days

FEATURE ENGINEERING TECHNIQUES:
- Statistical transformations: rolling means, standard deviations
- Temporal encoding: cyclical transformations for seasonal patterns
- Data quality indicators: completeness flags and imputation markers
- Lag creation: historical value incorporation for time series modeling

FEATURE CREATION IMPLEMENTATION:
```python
# From feature_store_pipeline.py
def create_temporal_features(df):
    """Create temporal features from timestamp"""
    df['day_of_week'] = df['date'].dt.dayofweek
    df['month'] = df['date'].dt.month
    df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)
    df['week_of_year'] = df['date'].dt.isocalendar().week
    df['day_of_year'] = df['date'].dt.dayofyear
    
    # Cyclical encoding for seasonality
    df['doy_sin'] = np.sin(2 * np.pi * df['day_of_year'] / 365.0)
    df['doy_cos'] = np.cos(2 * np.pi * df['day_of_year'] / 365.0)
    return df

def create_lag_features(df, target_col, lags=[1, 2, 3, 4, 5, 6, 7]):
    """Create lag features for time series prediction"""
    for lag in lags:
        df[f'{target_col}_lag{lag}'] = df[target_col].shift(lag)
    return df

def create_rolling_features(df, target_col, windows=[3, 7, 14, 30]):
    """Create rolling statistics features"""
    for window in windows:
        df[f'{target_col}_roll_mean_{window}'] = df[target_col].rolling(window, min_periods=max(2, window//3)).mean()
        df[f'{target_col}_roll_std_{window}'] = df[target_col].rolling(window, min_periods=max(2, window//3)).std()
    return df

def create_weather_features(df):
    """Create weather lag and rolling features"""
    for base_col in ["temp_mean", "humidity_mean", "wind_u_mean", "wind_v_mean"]:
        if base_col in df.columns:
            # Weather lags (1-3 days)
            for lag in [1, 2, 3]:
                df[f'{base_col}_lag{lag}'] = df[base_col].shift(lag)
            # Weather rolling stats (3, 7 days)
            for w in [3, 7]:
                df[f'{base_col}_roll_mean_{w}'] = df[base_col].rolling(window=w, min_periods=max(2, w//3)).mean()
                df[f'{base_col}_roll_std_{w}'] = df[base_col].rolling(window=w, min_periods=max(2, w//3)).std()
    return df

def create_change_rate_features(df):
    """Create change rate features"""
    df['aqi_change_rate'] = df['aqi_daily'] - df['aqi_daily'].shift(1)
    return df
```

================================================================
3. FEATURE STORE IMPLEMENTATION
================================================================

FEATURE STORE TECHNOLOGY: Feast (Open Source)

WHY FEAST:
1. Centralized Feature Management: Single source of truth for all features
2. Online/Offline Consistency: Ensures training and serving use identical features
3. Scalability: Handles large feature sets and high-throughput requests
4. Integration: Seamless integration with ML frameworks and data sources
5. Versioning: Built-in feature versioning and rollback capabilities
6. Real-time Serving: Low-latency feature retrieval for online predictions

IMPLEMENTATION DETAILS:

OFFLINE STORAGE:
- Format: Parquet files (efficient, compressed columnar storage)
- Location: Data/feature_store/karachi_daily_features.parquet
- Schema: 113 features with proper data types and constraints

ONLINE STORAGE:
- Database: SQLite (online.db) for real-time feature serving
- TTL: 2-day time-to-live for feature freshness
- Materialization: Configurable date ranges (default: last 90 days)

FEATURE VIEWS:
- Entity: Karachi city identifier (karachi_id)
- Schema: Comprehensive feature definitions with proper data types
- TTL: 2-day expiration for online features
- Join Keys: Entity-based feature retrieval

FEATURE VIEW DEFINITION:
```python
# From feature_repo/karachi_features.py
karachi_fv = FeatureView(
    name="karachi_air_quality_daily",
    entities=[karachi],
    ttl=timedelta(days=2),
    schema=[
        # Core daily means
        Field(name="aqi_daily", dtype=Float32),
        Field(name="pm2_5_mean", dtype=Float32),
        Field(name="pm10_mean", dtype=Float32),
        Field(name="co_mean", dtype=Float32),
        Field(name="no_mean", dtype=Float32),
        Field(name="no2_mean", dtype=Float32),
        Field(name="o3_mean", dtype=Float32),
        Field(name="so2_mean", dtype=Float32),
        Field(name="nh3_mean", dtype=Float32),
        
        # Calendar/time features
        Field(name="day_of_week", dtype=Float32),
        Field(name="day_of_year", dtype=Float32),
        Field(name="month", dtype=Float32),
        Field(name="is_weekend", dtype=Float32),
        Field(name="week_of_year", dtype=Float32),
        Field(name="doy_sin", dtype=Float32),
        Field(name="doy_cos", dtype=Float32),
        
        # Lags for AQI and particulates (1..7)
        Field(name="aqi_daily_lag1", dtype=Float32),
        Field(name="aqi_daily_lag2", dtype=Float32),
        Field(name="aqi_daily_lag3", dtype=Float32),
        Field(name="aqi_daily_lag4", dtype=Float32),
        Field(name="aqi_daily_lag5", dtype=Float32),
        Field(name="aqi_daily_lag6", dtype=Float32),
        Field(name="aqi_daily_lag7", dtype=Float32),
        
        # Rolling statistics
        Field(name="aqi_roll_mean_3", dtype=Float32),
        Field(name="aqi_roll_mean_7", dtype=Float32),
        Field(name="aqi_roll_mean_14", dtype=Float32),
        Field(name="aqi_roll_mean_30", dtype=Float32),
        Field(name="aqi_roll_std_3", dtype=Float32),
        Field(name="aqi_roll_std_7", dtype=Float32),
        Field(name="aqi_roll_std_14", dtype=Float32),
        Field(name="aqi_roll_std_30", dtype=Float32),
    ]
)
```

MATERIALIZATION PROCESS:
- Command: feast materialize [start_date] [end_date]
- Frequency: Automated via run_all.ps1 script
- Coverage: Last 90 days by default, configurable up to 3 years
- Consistency: Verification scripts ensure online/offline alignment

MATERIALIZATION COMMANDS:
```bash
# Full materialization for last 90 days
cd feature_repo
feast materialize 2025-05-15T00:00:00 2025-08-14T00:00:00

# Incremental materialization
feast materialize-incremental 2025-08-14T00:00:00

# Verify online/offline consistency
python verify_online_offline_consistency.py
```

================================================================
4. DATA QUALITY AND VALIDATION
================================================================

QUALITY CONTROLS:
- Minimum hourly observations per day (configurable, default: 20)
- Data completeness validation before feature generation
- Imputation strategies for short data gaps
- Duplicate detection and removal

DATA VALIDATION IMPLEMENTATION:
```python
# From feature_store_pipeline.py
def validate_data_quality(df, min_hours_per_day=20):
    """Validate data quality and completeness"""
    validation_results = {}
    
    # Check for minimum hourly observations per day
    hourly_counts = df.groupby(df['event_timestamp'].dt.date).size()
    incomplete_days = hourly_counts[hourly_counts < min_hours_per_day]
    validation_results['incomplete_days'] = len(incomplete_days)
    
    # Check for missing values in critical columns
    critical_cols = ['pm2_5', 'pm10', 'o3', 'no2', 'co', 'so2', 'nh3']
    missing_counts = df[critical_cols].isnull().sum()
    validation_results['missing_values'] = missing_counts.to_dict()
    
    # Check for outliers (values beyond 3 standard deviations)
    outlier_counts = {}
    for col in critical_cols:
        if col in df.columns:
            mean_val = df[col].mean()
            std_val = df[col].std()
            outliers = df[(df[col] < mean_val - 3*std_val) | 
                         (df[col] > mean_val + 3*std_val)]
            outlier_counts[col] = len(outliers)
    validation_results['outliers'] = outlier_counts
    
    return validation_results

def impute_missing_data(df, strategy='interpolate'):
    """Impute missing data using time-based interpolation"""
    if strategy == 'interpolate':
        # Interpolate single missing calendar days at daily granularity
        interp_cols = ["pm2_5_mean", "pm10_mean", "co_mean", "no_mean", 
                       "no2_mean", "o3_mean", "so2_mean", "nh3_mean", "aqi_daily"]
        daily = df.sort_values("date").set_index("date")
        before = daily[interp_cols].isna().copy()
        daily[interp_cols] = daily[interp_cols].interpolate(method='time', limit=1, limit_direction='both')
        filled_mask = before & daily[interp_cols].notna()
        any_filled = filled_mask.any(axis=1)
        daily.loc[any_filled, "imputed"] = 1
        df = daily.reset_index()
    
    return df
```

VALIDATION PROCESSES:
- Online vs offline consistency verification
- Feature schema validation
- Data type and range checking
- Temporal continuity validation

CONSISTENCY VERIFICATION:
```python
# From verify_online_offline_consistency.py
def verify_consistency():
    """Verify online vs offline feature consistency"""
    # Load offline features
    offline_df = pd.read_parquet("../Data/feature_store/karachi_daily_features.parquet")
    
    # Get online features for same dates
    store = FeatureStore(repo_path=".")
    online_features = store.get_online_features(
        features=store.list_features(),
        entity_rows=[{"karachi_id": 1}]
    ).to_dict()
    
    # Compare key metrics
    comparison = {
        'offline_count': len(offline_df),
        'online_count': len(online_features['event_timestamp']),
        'feature_count_offline': len(offline_df.columns),
        'feature_count_online': len(online_features.keys())
    }
    
    return comparison
```

ERROR HANDLING:
- API failure retry logic
- Data quality threshold enforcement
- Graceful degradation for missing data
- Comprehensive logging and monitoring

ERROR HANDLING IMPLEMENTATION:
```python
# From fetch_openweather_karachi.py
def fetch_with_retry(session, api_key, lat, lon, start_unix, end_unix, max_retries=3):
    """Fetch data with exponential backoff retry logic"""
    for attempt in range(max_retries):
        try:
            params = {
                "lat": lat, "lon": lon, 
                "start": start_unix, "end": end_unix, 
                "appid": api_key
            }
            resp = session.get(API_URL, params=params, timeout=30)
            
            if resp.status_code == 200:
                return resp.json()
            elif resp.status_code == 429:  # Rate limit
                wait_time = 2 ** attempt
                time.sleep(wait_time)
            elif resp.status_code in [500, 502, 503, 504]:  # Server errors
                wait_time = 2 ** attempt
                time.sleep(wait_time)
            else:
                raise RuntimeError(f"API error {resp.status_code}: {resp.text[:200]}")
                
        except Exception as e:
            if attempt == max_retries - 1:
                raise e
            wait_time = 2 ** attempt
            time.sleep(wait_time)
    
    raise RuntimeError("Max retries exceeded")
```

================================================================
5. AUTOMATION AND WORKFLOW
================================================================

AUTOMATION SCRIPT: run_all.ps1

WORKFLOW STEPS:
1. Dependency Installation: Python package verification and installation
2. Feature Generation: Automated pipeline execution with configurable parameters
3. Feast Registration: Feature store object updates and schema changes
4. Online Materialization: Feature store population for real-time serving
5. Consistency Verification: Online/offline data alignment validation
6. EDA Execution: Exploratory data analysis and model retraining
7. Model Training: Automated retraining with latest features

POWERSHELL AUTOMATION SCRIPT:
```powershell
# From run_all.ps1
Param(
  [int]$Days = 1095,
  [int]$MaterializeDays = 90,
  [switch]$ImputeShortGaps,
  [int]$MinHoursPerDay = 20
)

Set-StrictMode -Version Latest
$ErrorActionPreference = 'Stop'

function Invoke-Step {
  param(
    [string]$Message,
    [scriptblock]$Action
  )
  Write-Host "`n=== $Message ===" -ForegroundColor Cyan
  & $Action
}

# Move to repo root
Set-Location -Path $PSScriptRoot

Invoke-Step -Message "Install/verify Python dependencies" -Action {
  python -m pip install -r requirements.txt
}

Invoke-Step -Message "Build offline features (last $Days days)" -Action {
  $env:PYTHONUNBUFFERED = "1"
  $impute = ""
  if ($ImputeShortGaps) { $impute = "--impute_short_gaps" }
  python -u Data_Collection/feature_store_pipeline.py --days $Days $impute --min_hours_per_day $MinHoursPerDay
}

Invoke-Step -Message "Register Feast objects" -Action {
  Push-Location feature_repo
  feast apply
  Pop-Location
}

Invoke-Step -Message "Materialize features to Feast online store" -Action {
  $start = ((Get-Date).ToUniversalTime().AddDays(-1 * $MaterializeDays)).ToString('s') + 'Z'
  $end   = (Get-Date).ToUniversalTime().ToString('s') + 'Z'
  Push-Location feature_repo
  feast materialize $start $end
  Pop-Location
}
```

CONFIGURATION OPTIONS:
- Days: Feature generation range (default: 1095 days)
- Materialization: Online store coverage (default: 90 days)
- Imputation: Gap filling strategies
- Quality thresholds: Minimum data requirements

================================================================
6. DATA PIPELINE ARCHITECTURE
================================================================

COMPLETE DATA FLOW:
```
Raw APIs → Data Collection → Feature Engineering → Feature Store → Model Training → Web App
    ↓              ↓              ↓              ↓              ↓              ↓
OpenWeather    feature_store_  EDA/run_eda.py  Feast         train_*.py    FastAPI +
Open-Meteo     pipeline.py                    materialize    scripts       Gradio
```

DATA TRANSFORMATION PIPELINE:
```python
# From feature_store_pipeline.py
def main():
    args = parse_args()
    
    # 1. Fetch raw data from APIs
    raw_data = fetch_all_data(args.start, args.end, args.sleep, args.timeout)
    
    # 2. Process and clean raw data
    processed_data = process_raw_data(raw_data)
    
    # 3. Create daily aggregations
    daily_data = create_daily_aggregations(processed_data, args.min_hours_per_day)
    
    # 4. Engineer features
    features_df = engineer_features(daily_data)
    
    # 5. Handle missing data
    if args.impute_short_gaps:
        features_df = impute_missing_data(features_df)
    
    # 6. Save to feature store
    save_features(features_df, args.features_out)
    
    print(f"Feature generation complete. Saved to: {args.features_out}")

def fetch_all_data(start_date, end_date, sleep_sec, timeout):
    """Fetch data from all sources"""
    data = {}
    
    # Fetch OpenWeather air pollution data
    data['air_pollution'] = fetch_openweather_data(start_date, end_date, sleep_sec, timeout)
    
    # Fetch Open-Meteo meteorological data
    data['meteorological'] = fetch_openmeteo_data(start_date, end_date, sleep_sec, timeout)
    
    return data

def engineer_features(daily_df):
    """Create all engineered features"""
    # Temporal features
    daily_df = create_temporal_features(daily_df)
    
    # Lag features
    daily_df = create_lag_features(daily_df, 'aqi_daily')
    daily_df = create_lag_features(daily_df, 'pm2_5_mean')
    daily_df = create_lag_features(daily_df, 'pm10_mean')
    
    # Rolling statistics
    daily_df = create_rolling_features(daily_df, 'aqi_daily')
    daily_df = create_rolling_features(daily_df, 'pm2_5_mean')
    daily_df = create_rolling_features(daily_df, 'pm10_mean')
    
    # Meteorological aggregations
    daily_df = create_meteorological_features(daily_df)
    
    return daily_df
```

================================================================
7. INTEGRATION AND DEPLOYMENT
================================================================

MODEL INTEGRATION:
- Feature consistency between training and inference
- Real-time feature serving for online predictions
- Automated feature pipeline updates
- Model retraining with fresh features

FEATURE STORE INTEGRATION IN MODELS:
```python
# From WebApp/Backend/app/feast_client.py
class FeastClient:
    def __init__(self, repo_path="feature_repo"):
        self.store = FeatureStore(repo_path=repo_path)
        self.feature_refs = [
            "karachi_air_quality_daily:aqi_daily",
            "karachi_air_quality_daily:pm2_5_mean",
            "karachi_air_quality_daily:pm10_mean",
            "karachi_air_quality_daily:o3_mean",
            "karachi_air_quality_daily:no2_mean",
            "karachi_air_quality_daily:co_mean",
            "karachi_air_quality_daily:so2_mean",
            "karachi_air_quality_daily:nh3_mean",
            "karachi_air_quality_daily:day_of_week",
            "karachi_air_quality_daily:month",
            "karachi_air_quality_daily:is_weekend",
            "karachi_air_quality_daily:doy_sin",
            "karachi_air_quality_daily:doy_cos",
            "karachi_air_quality_daily:aqi_daily_lag1",
            "karachi_air_quality_daily:aqi_daily_lag2",
            "karachi_air_quality_daily:aqi_daily_lag3",
            "karachi_air_quality_daily:pm2_5_mean_lag1",
            "karachi_air_quality_daily:pm2_5_mean_lag2",
            "karachi_air_quality_daily:pm2_5_mean_lag3",
            "karachi_air_quality_daily:pm10_mean_lag1",
            "karachi_air_quality_daily:pm10_mean_lag2",
            "karachi_air_quality_daily:pm10_mean_lag3",
            "karachi_air_quality_daily:aqi_roll_mean_7",
            "karachi_air_quality_daily:aqi_roll_std_7",
            "karachi_air_quality_daily:pm25_roll_mean_7",
            "karachi_air_quality_daily:pm25_roll_std_7",
            "karachi_air_quality_daily:pm10_roll_mean_7",
            "karachi_air_quality_daily:pm10_roll_std_7"
        ]
    
    def get_latest_features(self):
        """Get latest features for prediction"""
        entity_df = pd.DataFrame([{"karachi_id": 1}])
        
        try:
            online_features = self.store.get_online_features(
                features=self.feature_refs,
                entity_rows=entity_df
            ).to_dict()
            return online_features
        except Exception as e:
            print(f"Error fetching online features: {e}")
            return None
```

DEPLOYMENT CONSIDERATIONS:
- Docker containerization for consistent environments
- Environment variable management for API keys
- Health monitoring and endpoint verification
- Automated deployment pipelines via GitHub Actions

DOCKER CONFIGURATION:
```dockerfile
# From Dockerfile
FROM python:3.9-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements and install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY . .

# Set environment variables
ENV PYTHONPATH=/app
ENV FEAST_REPO_PATH=/app/feature_repo

# Expose port
EXPOSE 8000

# Health check
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# Run the application
CMD ["python", "-m", "uvicorn", "WebApp.Backend.app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

================================================================
8. REAL-TIME UPDATE SYSTEM
================================================================

REAL-TIME UPDATE ARCHITECTURE:

The system now implements an automated real-time update system that continuously keeps data current:

AUTOMATED UPDATE WORKFLOW:
1. **Background Worker**: Runs every 30 minutes automatically when web app starts
2. **Smart Detection**: Only updates when data is actually outdated
3. **Gap Filling**: Automatically detects and fills missing date ranges
4. **Feature Pipeline**: Runs feature_store_pipeline.py with optimal date ranges
5. **Feast Materialization**: Updates online store with latest features
6. **Prediction Generation**: Creates fresh predictions for upcoming days

IMPLEMENTATION DETAILS:
```python
# From WebApp/Backend/app/main.py
def check_if_update_needed():
    """Check if real-time update is actually needed"""
    # Compares latest feature date with current date
    # Only updates if data is outdated
    
def run_feature_update():
    """Run the feature update pipeline with smart date range calculation"""
    # Dynamically calculates start_date and end_date
    # Fills gaps from latest_date + 1 to current_date
    # Ensures sufficient historical data for lag features
```

SMART DATE RANGE CALCULATION:
- **When data exists**: Only fetches missing gap (latest_date + 1 to current_date)
- **When no data exists**: Fetches 21 days to ensure enough data for lag features
- **Prevents unnecessary updates**: Skips update cycles when data is current
- **Handles edge cases**: Manages data gaps and ensures feature completeness

REAL-TIME FEATURES:
- **Automated Background Processing**: Updates run without blocking user interactions
- **Manual Trigger Support**: Users can force updates via web interface
- **Status Monitoring**: Real-time update status and next scheduled update
- **Error Handling**: Comprehensive error handling with graceful degradation
- **Logging**: Detailed logging for debugging and monitoring

================================================================
9. MONITORING AND MAINTENANCE
================================================================

MONITORING:
- Feature freshness tracking
- Data quality metrics
- API rate limit monitoring
- Model performance correlation with feature updates

MONITORING IMPLEMENTATION:
```python
# Health check endpoint
@app.get("/health")
async def health_check():
    """Health check for the entire system"""
    health_status = {
        "status": "healthy",
        "timestamp": datetime.utcnow().isoformat(),
        "components": {
            "feature_store": check_feature_store_health(),
            "models": check_model_health(),
            "data_freshness": check_data_freshness()
        }
    }
    
    # Check if any component is unhealthy
    if any(comp.get("status") == "unhealthy" for comp in health_status["components"].values()):
        health_status["status"] = "unhealthy"
        return JSONResponse(status_code=503, content=health_status)
    
    return health_status

def check_feature_store_health():
    """Check feature store health"""
    try:
        store = FeatureStore(repo_path="feature_repo")
        # Check if online store is accessible
        features = store.get_online_features(
            features=["karachi_air_quality_daily:aqi_daily"],
            entity_rows=[{"karachi_id": 1}]
        )
        return {"status": "healthy", "message": "Feature store accessible"}
    except Exception as e:
        return {"status": "unhealthy", "message": f"Feature store error: {str(e)}"}

def check_data_freshness():
    """Check if data is fresh (within last 2 days)"""
    try:
        store = FeatureStore(repo_path="feature_repo")
        features = store.get_online_features(
            features=["karachi_air_quality_daily:event_timestamp"],
            entity_rows=[{"karachi_id": 1}]
        )
        
        latest_timestamp = features["event_timestamp"][0]
        if latest_timestamp:
            latest_date = pd.to_datetime(latest_timestamp)
            days_old = (datetime.utcnow() - latest_date).days
            
            if days_old <= 2:
                return {"status": "healthy", "message": f"Data is {days_old} days old"}
            else:
                return {"status": "unhealthy", "message": f"Data is {days_old} days old (stale)"}
        else:
            return {"status": "unhealthy", "message": "No timestamp data available"}
    except Exception as e:
        return {"status": "unhealthy", "message": f"Data freshness check error: {str(e)}"}
```

MAINTENANCE:
- Regular feature store materialization
- Data quality threshold adjustments
- Feature engineering pipeline updates
- API key rotation and security updates

MAINTENANCE SCHEDULE:
```bash
# Daily maintenance (automated via cron or GitHub Actions)
0 2 * * * cd /path/to/10pearls && ./run_all.ps1 -Days 30 -MaterializeDays 7

# Weekly full materialization
0 3 * * 0 cd /path/to/10pearls/feature_repo && feast materialize 2025-01-01T00:00:00 2025-08-14T00:00:00

# Monthly data quality check
0 4 1 * * cd /path/to/10pearls && python -m EDA.run_eda
```

================================================================
9. TROUBLESHOOTING AND DEBUGGING
================================================================

COMMON ISSUES AND SOLUTIONS:

1. FEATURE STORE MATERIALIZATION FAILURES:
   ```bash
   # Problem: Materialization fails with date range errors
   # Solution: Check date format and ensure start < end
   feast materialize 2025-08-01T00:00:00 2025-08-14T00:00:00
   
   # Problem: Insufficient data for materialization
   # Solution: Check offline feature store has data for specified range
   ls -la Data/feature_store/
   ```

2. API RATE LIMITING:
   ```python
   # Problem: OpenWeather API returns 429 errors
   # Solution: Increase sleep intervals and implement exponential backoff
   def fetch_with_backoff(api_call_func, max_retries=5):
       for attempt in range(max_retries):
           try:
               return api_call_func()
           except requests.exceptions.HTTPError as e:
               if e.response.status_code == 429:
                   wait_time = min(60, 2 ** attempt)  # Cap at 60 seconds
                   time.sleep(wait_time)
               else:
                   raise e
   ```

3. DATA QUALITY ISSUES:
   ```python
   # Problem: Missing features in online store
   # Solution: Check feature view definitions match training data
   def debug_feature_mismatch():
       # Load training features
       train_features = pd.read_parquet("Data/feature_store/karachi_daily_features.parquet")
       print(f"Training features: {len(train_features.columns)}")
       
       # Check online features
       store = FeatureStore(repo_path="feature_repo")
       online_features = store.list_features()
       print(f"Online features: {len(online_features)}")
       
       # Compare feature names
       train_cols = set(train_features.columns)
       online_cols = set([f.name for f in online_features])
       missing = train_cols - online_cols
       print(f"Missing in online: {missing}")
   ```

4. MODEL LOADING ERRORS:
   ```python
   # Problem: Models not found in registry
   # Solution: Check file naming conventions and paths
   def debug_model_loading():
       import glob
       import os
       
       registry_path = "Models/registry"
       
       # Check LightGBM models
       lgb_models = glob.glob(f"{registry_path}/lgb_*.txt")
       print(f"LightGBM models: {lgb_models}")
       
       # Check HGBR models
       hgb_models = glob.glob(f"{registry_path}/hgb_*.joblib")
       print(f"HGBR models: {hgb_models}")
       
       # Check model metadata
       for model_path in lgb_models + hgb_models:
           print(f"{model_path}: {os.path.getmtime(model_path)}")
   ```

================================================================
10. FUTURE ENHANCEMENTS
================================================================

PLANNED IMPROVEMENTS:
- Additional data sources integration
- Real-time streaming capabilities
- Advanced feature selection algorithms
- Automated feature importance analysis
- Multi-city expansion capabilities

ENHANCEMENT ROADMAP:

1. SHORT TERM (1-3 months):
   - Implement real-time data streaming with Apache Kafka
   - Add automated feature selection using mutual information
   - Implement A/B testing for feature engineering approaches

2. MEDIUM TERM (3-6 months):
   - Integrate additional air quality monitoring stations
   - Implement automated anomaly detection in data pipeline
   - Add support for multiple cities and regions

3. LONG TERM (6+ months):
   - Implement federated learning for multi-city models
   - Add satellite imagery data integration
   - Implement automated model retraining based on performance drift

================================================================
11. GLOSSARY OF TERMS
================================================================

TECHNICAL TERMINOLOGY:

- **AQI**: Air Quality Index, EPA standard for air quality measurement
- **EPA**: Environmental Protection Agency, sets air quality standards
- **PM2.5/PM10**: Particulate matter with diameters ≤2.5/10 micrometers
- **Feast**: Open-source feature store for machine learning
- **Materialization**: Loading features from offline to online store
- **TTL**: Time To Live, feature expiration in online store
- **Feature View**: Feast concept defining feature schema and source
- **Entity**: Feast concept representing business objects (e.g., cities)
- **Lag Features**: Historical values used for time series prediction
- **Rolling Statistics**: Moving averages and standard deviations over time windows
- **Cyclical Encoding**: Sine/cosine transformations for seasonal patterns
- **Imputation**: Filling missing data using various strategies

================================================================
END OF DOCUMENTATION
================================================================
