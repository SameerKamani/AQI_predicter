WEBAPP DEPLOYMENT AND CI/CD DOCUMENTATION
============================================

PROJECT: Karachi Air Quality Index (AQI) Prediction System
DATE: August 15, 2025
VERSION: 1.0

============================================
1. WEBAPP ARCHITECTURE OVERVIEW
============================================

ARCHITECTURE PATTERN: FastAPI Backend + Gradio Frontend Integration

OVERALL STRUCTURE:
```
WebApp/
├── Backend/
│   ├── app/
│   │   ├── main.py              # FastAPI application server
│   │   ├── model_loader.py      # Model registry and prediction logic
│   │   ├── feast_client.py      # Feature store integration
│   │   └── __init__.py
│   └── __init__.py
├── Frontend/
│   └── gradio_app.py            # Gradio interface (mounted in FastAPI)
└── __init__.py
```

WHY THIS ARCHITECTURE:
1. **FastAPI Backend**: High-performance async API with automatic OpenAPI documentation
2. **Gradio Integration**: Rapid UI prototyping and user interaction
3. **Unified Deployment**: Single container serving both API and UI
4. **Scalability**: Async support for high-concurrency prediction requests

CURRENT WEBAPP STRUCTURE ALIGNMENT:

**Docker Configuration Alignment**:
- **Working Directory**: `/app` in container maps to project root
- **Import Paths**: `WebApp.Backend.app.main:app` correctly references the structure
- **File Access**: All WebApp components accessible from container root
- **Feature Store**: `feature_repo/` accessible for Feast operations
- **Model Registry**: `Models/registry/` accessible for model loading

**File Structure Verification**:
```bash
# Container file structure
/app/
├── WebApp/
│   ├── Backend/
│   │   └── app/
│   │       ├── main.py          # FastAPI app entry point
│   │       ├── model_loader.py  # Model loading logic
│   │       └── feast_client.py  # Feature store client
│   └── Frontend/
│       └── gradio_app.py        # Gradio interface
├── feature_repo/                 # Feast configuration
├── Models/registry/              # Trained models
├── Data/feature_store/           # Feature data
└── requirements.txt              # Python dependencies
```

**Import Resolution**:
```python
# In main.py - correctly imports from WebApp structure
from WebApp.Backend.app import feast_client
from WebApp.Backend.app import model_loader

# In gradio_app.py - correctly imports from WebApp structure
from WebApp.Frontend.gradio_app import demo
```

============================================
2. BACKEND IMPLEMENTATION (FastAPI)
============================================

REAL-TIME UPDATE SYSTEM:

The backend implements a comprehensive real-time update system that automatically keeps data and predictions current:

```python
# Smart update checking function
def check_if_update_needed():
    """Check if real-time update is actually needed"""
    try:
        import pandas as pd
        from datetime import datetime, timezone
        
        # Check feature store for latest data
        features_path = Path(FEATURES_PARQUET)
        if not features_path.exists():
            print("📁 Feature store not found - update needed")
            return True
        
        # Read latest feature data
        df = pd.read_parquet(features_path)
        if df.empty:
            print("📊 Feature store is empty - update needed")
            return True
        
        # Get latest timestamp from feature store
        latest_feature_date = pd.to_datetime(df['event_timestamp'].max()).date()
        current_date = datetime.now(timezone.utc).date()
        
        print(f"📅 Latest feature date: {latest_feature_date}")
        print(f"📅 Current date: {current_date}")
        
        # Check if we have today's data
        if latest_feature_date >= current_date:
            print("✅ Features are up-to-date - no update needed")
            return False
        else:
            print(f"🔄 Features are outdated (last: {latest_feature_date}, current: {current_date}) - update needed")
            return True
            
    except Exception as e:
        print(f"❌ Error checking update status: {e}")
        return True  # Default to updating if check fails

# Feature update pipeline
def run_feature_update():
    """Run the feature update pipeline with smart date range calculation"""
    try:
        print("🔄 Starting feature update pipeline...")
        
        # Check if update is needed first
        if not check_if_update_needed():
            print("✅ No update needed - features are current")
            return True
        
        # Determine date range for fetching
        features_path = Path(FEATURES_PARQUET)
        if features_path.exists():
            df = pd.read_parquet(features_path)
            if not df.empty:
                latest_date = pd.to_datetime(df['event_timestamp'].max()).date()
                current_date = datetime.now(timezone.utc).date()
                
                # Fill the gap from latest_date + 1 to current_date
                start_date = (latest_date + timedelta(days=1)).strftime("%Y-%m-%d")
                end_date = current_date.strftime("%Y-%m-%d")
                
                print(f"📅 Latest data: {latest_date}")
                print(f"📅 Current date: {current_date}")
                print(f"🔄 Filling gap from {start_date} to {end_date}...")
            else:
                # No existing data, fetch last 21 days to ensure enough data for features and lags
                start_date = (datetime.now(timezone.utc) - timedelta(days=21)).strftime("%Y-%m-%d")
                end_date = datetime.now(timezone.utc).strftime("%Y-%m-%d")
                print(f"🔄 No existing data, fetching from {start_date} to {end_date} (including lag days)...")
        else:
            # No existing file, fetch last 21 days to ensure enough data for features and lags
            start_date = (datetime.now(timezone.utc) - timedelta(days=21)).strftime("%Y-%m-%d")
            end_date = datetime.now(timezone.utc).strftime("%Y-%m-%d")
            print(f"🔄 No feature store found, fetching from {start_date} to {end_date} (including lag days)...")
        
        # Run feature pipeline with proper date range
        result = subprocess.run([
            "python", "Data_Collection/feature_store_pipeline.py",
            "--start", start_date,
            "--end", end_date,
            "--impute_short_gaps",
            "--min_hours_per_day", "16",
            "--append"
        ], capture_output=True, text=True, cwd=_ROOT)
        
        if result.returncode != 0:
            print(f"❌ Feature pipeline failed: {result.stderr}")
            return False
        
        print("✅ Feature pipeline completed successfully")
        
        # Materialize Feast with proper date range
        start_datetime = f"{start_date}T00:00:00Z"
        end_datetime = f"{end_date}T23:59:59Z"
        print(f"🔄 Materializing from {start_datetime} to {end_datetime}...")
        
        materialize_result = subprocess.run([
            "feast", "materialize", start_datetime, end_datetime
        ], capture_output=True, text=True, cwd=FEAST_REPO_PATH)
        
        if materialize_result.returncode != 0:
            print(f"❌ Feast materialize failed: {materialize_result.stderr}")
            return False
        
        print("✅ Feast materialization completed successfully")
        return True
        
    except Exception as e:
        print(f"❌ Feature update failed: {e}")
        return False

# Prediction update pipeline
def run_prediction_update():
    """Run the prediction update pipeline"""
    try:
        print("🔄 Starting prediction update pipeline...")
        
        result = subprocess.run([
            "python", "Models/predict_realtime.py"
        ], capture_output=True, text=True, cwd=_ROOT)
        
        if result.returncode != 0:
            print(f"❌ Prediction pipeline failed: {result.stderr}")
            return False
        
        print("✅ Prediction pipeline completed successfully")
        return True
        
    except Exception as e:
        print(f"❌ Prediction update failed: {e}")
        return False

# Background worker for automated updates
def realtime_update_worker():
    """Background worker for smart real-time updates"""
    while True:
        try:
            print(f"🕐 Real-time update cycle starting at {datetime.now()}")
            
            # Check if update is needed first
            if check_if_update_needed():
                print("🔄 Update needed - running full pipeline...")
                # Update features
                if run_feature_update():
                    # Generate predictions
                    run_prediction_update()
            else:
                print("✅ No update needed - skipping this cycle")
            
            # Wait for next cycle
            time.sleep(UPDATE_INTERVAL_MINUTES * 60)
            
        except Exception as e:
            print(f"❌ Real-time update cycle failed: {e}")
            time.sleep(60)  # Wait 1 minute before retrying
```

AUTOMATED UPDATE WORKFLOW:

1. **Startup**: Real-time update worker starts automatically when the application starts
2. **Interval Checks**: Every 30 minutes (configurable), the system checks if updates are needed
3. **Smart Detection**: Only updates when data is actually outdated
4. **Gap Filling**: Automatically detects and fills missing date ranges
5. **Background Processing**: Updates run in background threads without blocking the API
6. **Manual Triggers**: Users can force updates via the `/update/realtime` endpoint

MAIN APPLICATION SERVER (main.py):

CORE COMPONENTS:
```python
# FastAPI app configuration
app = FastAPI(title="AQI Prediction Service", version="1.0.0")

# CORS middleware for cross-origin requests
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
```

ENVIRONMENT CONFIGURATION:
```python
# Load environment variables from .env file
load_dotenv('.env')

# Environment variable configuration with sensible defaults
_ROOT = str(Path(__file__).resolve().parents[3])  # Repository root
FEAST_REPO_PATH = _get_env_path("FEAST_REPO_PATH", 
                                os.path.join(_ROOT, "feature_repo"))
FEATURES_PARQUET = _get_env_path("FEATURES_PARQUET", 
                                 os.path.join(_ROOT, "Data", "feature_store", 
                                            "karachi_daily_features.parquet"))
REGISTRY_DIR = _get_env_path("REGISTRY_DIR", 
                             os.path.join(_ROOT, "Models", "registry"))

# Real-time update configuration
UPDATE_INTERVAL_MINUTES = int(os.getenv("UPDATE_INTERVAL_MINUTES", "30"))
REALTIME_UPDATE_ENABLED = os.getenv("REALTIME_UPDATE_ENABLED", "true").lower() == "true"
```

STARTUP EVENTS:
```python
@app.on_event("startup")
def _startup() -> None:
    """Initialize application on startup"""
    # Initialize model registry on startup for low-latency requests
    model_loader.initialize_registry(REGISTRY_DIR)
    
    # Start real-time update worker if enabled
    if REALTIME_UPDATE_ENABLED:
        print(f"🚀 Starting real-time update worker (interval: {UPDATE_INTERVAL_MINUTES} minutes)")
        update_thread = threading.Thread(
            target=realtime_update_worker,
            daemon=True
        )
        update_thread.start()
        print("✅ Real-time update worker started successfully")
    else:
        print("⚠️ Real-time updates are disabled")
```

API ENDPOINTS:

1. **Health Check Endpoint**:
```python
@app.get("/health")
def health() -> Dict[str, Any]:
    latest_ts: Optional[str] = None
    try:
        row = feast_client.get_latest_offline_row(FEATURES_PARQUET)
        if row is not None and "event_timestamp" in row.index:
            latest_ts = str(row["event_timestamp"])
    except Exception:
        latest_ts = None

    artifacts = model_loader.current_artifacts_summary()
    return {
        "status": "ok",
        "latest_feature_timestamp": latest_ts,
        "artifacts": artifacts,
    }
```

2. **Latest Features Endpoint**:
```python
@app.get("/features/latest")
def features_latest() -> Dict[str, Any]:
    try:
        row = feast_client.get_latest_features(FEAST_REPO_PATH, FEATURES_PARQUET)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to load features: {e}")
    if row is None:
        raise HTTPException(status_code=404, detail="No features available")
    
    # Convert Series to dict for JSON serialization
    return {"features": {k: (None if pd.isna(v) else 
                            (str(v) if hasattr(v, "isoformat") else v)) 
                        for k, v in row.to_dict().items()}}
```

3. **Prediction Endpoint**:
```python
@app.get("/predict/latest")
def predict_latest() -> Dict[str, Any]:
    # Get latest features from Feast
    row = feast_client.get_latest_features(FEAST_REPO_PATH, FEATURES_PARQUET)
    if row is None:
        raise HTTPException(status_code=404, detail="No features available for prediction")
    
    try:
        # Make prediction using loaded models
        pred = model_loader.predict_all_from_series(row)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Prediction failed: {e}")
    
    return pred

4. **Real-Time Update Endpoint**:
```python
@app.post("/update/realtime")
def trigger_realtime_update(background_tasks: BackgroundTasks) -> Dict[str, Any]:
    """Trigger real-time update manually - only if needed"""
    try:
        print("🚀 Manual real-time update triggered")
        
        # Check if update is actually needed
        if not check_if_update_needed():
            return {
                "status": "skipped",
                "message": "No update needed - features are current",
                "timestamp": datetime.now().isoformat(),
                "next_update": (datetime.now() + timedelta(minutes=UPDATE_INTERVAL_MINUTES)).isoformat()
            }
        
        print("🔄 Update needed - starting pipeline...")
        
        # Add update tasks to background
        background_tasks.add_task(run_feature_update)
        background_tasks.add_task(run_prediction_update)
        
        return {
            "status": "success",
            "message": "Real-time update started",
            "timestamp": datetime.now().isoformat(),
            "next_update": (datetime.now() + timedelta(minutes=UPDATE_INTERVAL_MINUTES)).isoformat()
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Update failed: {e}")

5. **Update Status Endpoint**:
```python
@app.get("/update/status")
def update_status() -> Dict[str, Any]:
    """Get real-time update configuration status"""
    return {
        "realtime_updates_enabled": REALTIME_UPDATE_ENABLED,
        "update_interval_minutes": UPDATE_INTERVAL_MINUTES,
        "next_scheduled_update": (datetime.now() + timedelta(minutes=UPDATE_INTERVAL_MINUTES)).isoformat(),
        "feature_store_path": FEAST_REPO_PATH,
        "models_registry_path": REGISTRY_DIR
    }

6. **Historical Data Endpoint**:
```python
@app.get("/series/last30")
def get_last_30_days() -> Dict[str, Any]:
    """Get last 30 days of historical AQI data for charting"""
    try:
        df = pd.read_parquet(FEATURES_PARQUET)
        if df.empty:
            raise HTTPException(status_code=404, detail="No historical data available")
        
        # Get last 30 days
        latest_date = pd.to_datetime(df['event_timestamp'].max())
        start_date = latest_date - timedelta(days=29)
        
        recent_data = df[
            pd.to_datetime(df['event_timestamp']) >= start_date
        ].sort_values('event_timestamp')
        
        # Format for charting
        chart_data = {
            'dates': recent_data['event_timestamp'].dt.strftime('%Y-%m-%d').tolist(),
            'aqi_values': recent_data['aqi_daily'].tolist()
        }
        
        return chart_data
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to load historical data: {e}")
```

GRADIO INTEGRATION:
```python
# Mount Gradio app from separate module
from WebApp.Frontend.gradio_app import demo

# Mount to FastAPI at root path
app = gr.mount_gradio_app(app, demo, path="/")

# Add prediction endpoint alias
@app.get("/predict")
def predict_endpoint():
    """Alias for /predict/latest for Gradio compatibility"""
    return predict_latest()
```

============================================
3. FRONTEND IMPLEMENTATION (Gradio)
============================================

REAL-TIME UPDATE INTEGRATION:

The Gradio frontend is fully integrated with the backend's real-time update system:

```python
# Real-time update trigger function
def trigger_realtime_update():
    """Trigger real-time update from frontend"""
    try:
        response = requests.post("http://localhost:8000/update/realtime")
        if response.status_code == 200:
            result = response.json()
            
            if result.get('status') == 'skipped':
                status_message = f"ℹ️ **{result['message']}**\n\n"
                status_message += f"**Timestamp:** {result['timestamp']}\n"
                status_message += f"**Next Update:** {result['next_update']}\n\n"
                status_message += "💡 Your data is already current - no update needed!"
            else:
                status_message = f"✅ **Real-time update triggered successfully!**\n\n"
                status_message += f"**Timestamp:** {result['timestamp']}\n"
                status_message += f"**Next Update:** {result['next_update']}\n\n"
                status_message += "🔄 Features and predictions are being updated in the background..."
                
                # Wait a moment for the update to complete, then refresh the chart
                import time
                time.sleep(2)
        else:
            status_message = f"❌ **Update failed with status {response.status_code}**"
            
        return status_message
    except Exception as e:
        return f"❌ **Error triggering update: {str(e)}**"

# Connect button to function
realtime_update_btn.click(
    fn=trigger_realtime_update,
    outputs=update_status
)
```

FRONTEND-BACKEND COMMUNICATION:

1. **Prediction Fetching**: Frontend calls `/predict` endpoint to get latest predictions
2. **Historical Data**: Frontend calls `/series/last30` endpoint for chart data
3. **Real-Time Updates**: Frontend calls `/update/realtime` to trigger manual updates
4. **Status Monitoring**: Frontend displays update status and next scheduled update

GRADIO INTERFACE DESIGN:

CURRENT IMPLEMENTATION:
```python
# WebApp/Frontend/gradio_app.py
# Comprehensive Gradio interface with real-time AQI predictions and visualizations
```

INTEGRATION APPROACH:
- **Mounted Integration**: Gradio app is mounted directly into FastAPI at root `/` endpoint
- **Unified Deployment**: Single server serves both API and UI
- **Real-Time Updates**: Integrated with backend real-time update system
- **Dynamic Data**: Live chart updates and real-time prediction refresh

ENHANCED UI FEATURES:

**Improved Styling and Layout:**
- **Centered Headings**: All section headers are centered with larger, bold fonts
- **Enhanced Typography**: Main heading (3.5em), section headings (2.2em)
- **Better Color Scheme**: Improved AQI colors, especially darker yellow for moderate levels
- **Responsive Design**: Grid-based layout that adapts to different screen sizes
- **Interactive Elements**: Hover effects on prediction boxes with smooth transitions

**Real-Time UI Updates:**
- **Dynamic Date Calculation**: Prediction boxes always show actual upcoming dates
- **Live Chart Updates**: Chart automatically fetches latest 30 days of data
- **Smart Data Fallbacks**: API → Parquet → Dummy data priority system
- **Status Monitoring**: Real-time update status display with timestamps
- **Manual Controls**: Force update and refresh buttons for user control

CURRENT FEATURES:
```python
# Real-time AQI prediction boxes with color coding and dynamic dates
def create_aqi_prediction_boxes(hd1, hd2, hd3):
    """Generate HTML for three color-coded AQI prediction boxes"""
    # Dynamic date calculation for forecast dates
    current_date = datetime.now()
    forecast_dates = [
        (current_date + timedelta(days=i)).strftime("%b %d") 
        for i in range(1, 4)
    ]
    
    # AQI category and color mapping with improved colors
    def get_aqi_color(aqi_value):
        if aqi_value <= 50: return "#00E400"      # Good - Green
        elif aqi_value <= 100: return "#D4AF37"   # Moderate - Darker Yellow
        elif aqi_value <= 150: return "#FF7E00"   # Unhealthy for Sensitive - Orange
        elif aqi_value <= 200: return "#FF0000"   # Unhealthy - Red
        elif aqi_value <= 300: return "#8F3F97"   # Very Unhealthy - Purple
        else: return "#7E0023"                    # Hazardous - Maroon
    
    # Generate HTML boxes with enhanced styling and hover effects
    html_content = f"""
    <div style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 20px; margin: 20px 0;">
        <div style="
            background: {get_aqi_color(hd1)};
            color: white;
            padding: 25px;
            border-radius: 15px;
            text-align: center;
            box-shadow: 0 8px 25px rgba(0,0,0,0.15);
            border: 3px solid {get_aqi_color(hd1)};
            transition: transform 0.3s ease;
        " onmouseover="this.style.transform='scale(1.05)'" onmouseout="this.style.transform='scale(1)'">
            <h3 style="margin: 0 0 10px 0; font-size: 18px; font-weight: 600;">{forecast_dates[0]}</h3>
            <div style="font-size: 32px; font-weight: bold; margin: 10px 0;">{hd1:.1f}</div>
            <div style="font-size: 14px; opacity: 0.9;">{get_aqi_category(hd1)}</div>
        </div>
        
        <div style="
            background: {get_aqi_color(hd2)};
            color: white;
            padding: 25px;
            border-radius: 15px;
            text-align: center;
            box-shadow: 0 8px 25px rgba(0,0,0,0.15);
            border: 3px solid {get_aqi_color(hd2)};
            transition: transform 0.3s ease;
        " onmouseover="this.style.transform='scale(1.05)'" onmouseout="this.style.transform='scale(1)'">
            <h3 style="margin: 0 0 10px 0; font-size: 18px; font-weight: 600;">{forecast_dates[1]}</h3>
            <div style="font-size: 32px; font-weight: bold; margin: 10px 0;">{hd2:.1f}</div>
            <div style="font-size: 14px; opacity: 0.9;">{get_aqi_category(hd2)}</div>
        </div>
        
        <div style="
            background: {get_aqi_color(hd3)};
            color: white;
            padding: 25px;
            border-radius: 15px;
            text-align: center;
            box-shadow: 0 8px 25px rgba(0,0,0,0.15);
            border: 3px solid {get_aqi_color(hd3)};
            transition: transform 0.3s ease;
        " onmouseover="this.style.transform='scale(1.05)'" onmouseout="this.style.transform='scale(1)'">
            <h3 style="margin: 0 0 10px 0; font-size: 18px; font-weight: bold;">{forecast_dates[2]}</h3>
            <div style="font-size: 32px; font-weight: bold; margin: 10px 0;">{hd3:.1f}</div>
            <div style="font-size: 14px; opacity: 0.9;">{get_aqi_category(hd3)}</div>
        </div>
    </div>
    """
    return html_content

# Interactive 30-day historical chart with real-time data fetching
def create_aqi_chart(hd1, hd2, hd3):
    """Create interactive chart with real-time historical data and predictions"""
    try:
        # Priority 1: Fetch from backend API for latest data
        response = requests.get("http://localhost:8000/series/last30")
        if response.status_code == 200:
            data = response.json()
            if data.get("points"):
                # Use API data as primary source
                api_dates = [pd.to_datetime(point["event_timestamp"]) for point in data["points"]]
                api_aqi = [point["aqi_daily"] for point in data["points"] if point["aqi_daily"] is not None]
                if api_dates and api_aqi:
                    dates = pd.to_datetime(api_dates)
                    historical_aqi = np.array(api_aqi)
                    print(f"✅ Using API data: {len(dates)} points from {dates.min()} to {dates.max()}")
                else:
                    raise Exception("No valid API data")
            else:
                raise Exception("No points in API response")
        else:
            raise Exception(f"API request failed: {response.status_code}")
    except Exception as api_error:
        print(f"⚠️ API fetch failed: {api_error}, falling back to parquet")
        
        # Priority 2: Fallback to local parquet file
        features_path = Path(__file__).parent.parent.parent / "Data" / "feature_store" / "karachi_daily_features.parquet"
        
        if features_path.exists():
            df = pd.read_parquet(features_path)
            df = df.sort_values("event_timestamp").tail(30)
            
            dates = pd.to_datetime(df["event_timestamp"])
            historical_aqi = df["aqi_daily"].values
            print(f"📁 Using parquet data: {len(dates)} points from {dates.min()} to {dates.max()}")
        else:
            # Priority 3: Generate dummy data if nothing works
            dates = pd.date_range(end=datetime.now(), periods=30, freq='D')
            historical_aqi = np.full(30, 70)
            print("⚠️ No data sources available, using dummy data")
    
    # Ensure exactly 30 data points for consistent chart display
    if len(dates) > 30:
        dates = dates[-30:]
        historical_aqi = historical_aqi[-30:]
    elif len(dates) < 30:
        # Pad with dummy data if we have less than 30 points
        missing_days = 30 - len(dates)
        if missing_days > 0:
            last_date = dates[-1] if len(dates) > 0 else datetime.now()
            dummy_dates = pd.date_range(start=last_date + timedelta(days=1), periods=missing_days, freq='D')
            dummy_aqi = np.full(missing_days, historical_aqi[-1] if len(historical_aqi) > 0 else 70)
            
            dates = np.concatenate([dates, dummy_dates])
            historical_aqi = np.concatenate([historical_aqi, dummy_aqi])
    
    # Add forecast points
    today = datetime.now()
    forecast_dates = [
        (today + timedelta(days=1)).strftime('%Y-%m-%d'),
        (today + timedelta(days=2)).strftime('%Y-%m-%d'),
        (today + timedelta(days=3)).strftime('%Y-%m-%d')
    ]
    
    # Create Plotly figure
    fig = go.Figure()
    
    # Historical data
    fig.add_trace(go.Scatter(
        x=dates,
        y=aqi_values,
        mode='lines+markers',
        name='Historical AQI',
        line=dict(color='blue', width=2),
        marker=dict(size=6)
    ))
    
    # Forecast points
    fig.add_trace(go.Scatter(
        x=forecast_dates,
        y=[hd1, hd2, hd3],
        mode='markers',
        name='Forecast',
        marker=dict(color='red', size=10, symbol='diamond'),
        text=[f'Day 1: {hd1:.1f}', f'Day 2: {hd2:.1f}', f'Day 3: {hd3:.1f}'],
        textposition='top center'
    ))
    
    fig.update_layout(
        title='Karachi AQI - 30 Days Historical + 3 Days Forecast',
        xaxis_title='Date',
        yaxis_title='AQI',
        hovermode='x unified',
        template='plotly_white'
    )
    
    return fig

# Model selection dropdown
model_selection = gr.Dropdown(
    choices=["Ensemble (Best)", "LightGBM", "HGBR", "Linear", "Random Forest"],
    value="Ensemble (Best)",
    label="Select Model",
    info="Ensemble combines all models for best accuracy"
)

# Real-time update button
realtime_update_btn = gr.Button("🚀 Force Real-Time Update", variant="secondary", size="lg")

# Manual prediction refresh
refresh_btn = gr.Button("🔄 Refresh Predictions", variant="primary", size="lg")
```

============================================
4. DOCKER CONFIGURATION
============================================

DOCKERFILE ANALYSIS:

BASE IMAGE SELECTION:
```dockerfile
FROM python:3.11-slim
```
**Why Python 3.11-slim**: 
- Latest stable Python version with security updates
- Slim variant reduces image size (~40MB vs ~100MB)
- Official image with regular security patches

ENVIRONMENT VARIABLES:
```dockerfile
ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    PIP_NO_CACHE_DIR=1
```
**Purpose**:
- `PYTHONDONTWRITEBYTECODE`: Prevents .pyc file creation
- `PYTHONUNBUFFERED`: Ensures real-time log output
- `PIP_NO_CACHE_DIR`: Reduces image size by not caching pip packages

SYSTEM DEPENDENCIES:
```dockerfile
RUN apt-get update \
    && apt-get install -y --no-install-recommends libgomp1 curl \
    && rm -rf /var/lib/apt/lists/*
```
**Why libgomp1 and curl**: 
- `libgomp1`: Required for LightGBM (OpenMP runtime)
- `curl`: Required for health check functionality
- `--no-install-recommends`: Minimizes package installation
- `rm -rf /var/lib/apt/lists/*`: Reduces image size

DEPENDENCY INSTALLATION:
```dockerfile
# Install python deps first for better layer caching
COPY requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt

# Install Feast for feature store integration
RUN pip install --no-cache-dir feast[sqlite]
```
**Layer Caching Strategy**:
- Copy requirements.txt first for better Docker layer caching
- `--no-cache-dir`: Reduces image size
- **Feast Installation**: Explicitly installs Feast with SQLite support for feature store
- Dependencies change less frequently than application code

APPLICATION DEPLOYMENT:
```dockerfile
# Copy the application code
COPY . .

# Create non-root user for security
RUN useradd --create-home --shell /bin/bash app && chown -R app:app /app
USER app
WORKDIR /app

# Expose API port
EXPOSE 8000

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# Default command - updated to match your WebApp structure
CMD ["uvicorn", "WebApp.Backend.app.main:app", "--host", "0.0.0.0", "--port", "8000", "--reload"]
```

**Security and Health Check Features**:
- **Non-root user**: Creates `app` user for security
- **Proper permissions**: Ensures app user owns all application files
- **Health checks**: Monitors application health via `/health` endpoint
- **Working directory**: Maintains `/app` as working directory for proper file access

DOCKER BUILD AND RUN COMMANDS:
```bash
# Build the image
docker build -t aqi-predictor .

# Run the container
docker run -p 8000:8000 \
  -e OPENWEATHER_API_KEY=your_key_here \
  -e FEAST_REPO_PATH=/app/feature_repo \
  aqi-predictor

# Run with volume mounts for development
docker run -p 8000:8000 \
  -v $(pwd)/feature_repo:/app/feature_repo \
  -v $(pwd)/Models/registry:/app/Models/registry \
  aqi-predictor

# Run with health check monitoring
docker run -p 8000:8000 \
  --health-cmd="curl -f http://localhost:8000/health || exit 1" \
  --health-interval=30s \
  --health-timeout=10s \
  --health-retries=3 \
  aqi-predictor
```

DOCKER OPTIMIZATION RECOMMENDATIONS:

1. **Multi-stage Builds**:
```dockerfile
# Build stage
FROM python:3.11-slim as builder
WORKDIR /app
COPY requirements.txt .
RUN pip install --user -r requirements.txt
RUN pip install --user feast[sqlite]

# Runtime stage
FROM python:3.11-slim
WORKDIR /app
COPY --from=builder /root/.local /root/.local
COPY . .
ENV PATH=/root/.local/bin:$PATH
RUN apt-get update \
    && apt-get install -y --no-install-recommends libgomp1 curl \
    && rm -rf /var/lib/apt/lists/*
RUN useradd --create-home --shell /bin/bash app && chown -R app:app /app
USER app
EXPOSE 8000
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1
CMD ["uvicorn", "WebApp.Backend.app.main:app", "--host", "0.0.0.0", "--port", "8000", "--reload"]
```

2. **Production Optimizations**:
```dockerfile
# Add production-specific optimizations
ENV PYTHONOPTIMIZE=1 \
    PYTHONHASHSEED=random

# Add security scanning
RUN pip install --no-cache-dir safety
RUN safety check

# Add production health checks
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1
```

3. **Environment Variable Management**:
```dockerfile
# Set environment variables for configuration
ENV PYTHONPATH=/app \
    FEAST_REPO_PATH=/app/feature_repo \
    MODEL_REGISTRY_PATH=/app/Models/registry \
    FEATURES_PARQUET=/app/Data/feature_store/karachi_daily_features.parquet

# Allow runtime override
ENV REALTIME_UPDATE_ENABLED=true \
    UPDATE_INTERVAL_MINUTES=30
```

2. **Security Enhancements** (Already implemented):
```dockerfile
# Create non-root user
RUN useradd --create-home --shell /bin/bash app
USER app
WORKDIR /app

# Copy application as non-root user
COPY --chown=app:app . .
```

3. **Production Optimizations**:
```dockerfile
# Add production-specific optimizations
ENV PYTHONOPTIMIZE=1 \
    PYTHONHASHSEED=random

# Add security scanning
RUN pip install --no-cache-dir safety
RUN safety check

# Add production health checks
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1
```

============================================
5. GITHUB ACTIONS CI/CD WORKFLOWS
============================================

WORKFLOW OVERVIEW:

1. **Features - Hourly Workflow** (`features-hourly.yml`)
2. **Train - Daily Workflow** (`train-daily.yml`)

FEATURES HOURLY WORKFLOW:

TRIGGER CONFIGURATION:
```yaml
on:
  schedule:
    - cron: '0 * * * *'  # Every hour at minute 0
  workflow_dispatch:      # Manual trigger capability
```

CONCURRENCY CONTROL:
```yaml
concurrency:
  group: features-hourly
  cancel-in-progress: true  # Cancel running jobs when new ones start
```

JOB STEPS:
```yaml
jobs:
  build-features:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Build features (today, append)
        env:
          OPENWEATHER_API_KEY: ${{ secrets.OPENWEATHER_API_KEY }}
        run: |
          python Data_Collection/feature_store_pipeline.py \
            --start "$(date -u +%Y-%m-%d)" \
            --end   "$(date -u +%Y-%m-%d)" \
            --impute_short_gaps \
            --min_hours_per_day 16 \
            --append

      - name: Feast apply + materialize last 1 day
        working-directory: feature_repo
        shell: bash
        run: |
          feast apply
          START=$(date -u -d "-1 day" +"%Y-%m-%dT%H:%M:%SZ")
          END=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
          feast materialize "$START" "$END"
```

FEATURE VALIDATION:
```yaml
- name: Quick sanity check
  run: |
    python - << 'PY'
    import pandas as pd
    df = pd.read_csv('Data/feature_store/karachi_daily_features.csv')
    neg = {c:int((df[c]<0).sum()) for c in ['pm2_5_mean','pm10_mean','co_mean','no_mean','no2_mean','o3_mean','so2_mean','nh3_mean'] if c in df.columns}
    print('NEGATIVE_COUNTS:', neg)
    PY
```

TRAIN DAILY WORKFLOW:

TRIGGER CONFIGURATION:
```yaml
on:
  schedule:
    - cron: '15 2 * * *'  # Daily at 2:15 AM UTC
  workflow_dispatch:        # Manual trigger capability
```

CONCURRENCY CONTROL:
```yaml
concurrency:
  group: train-daily
  cancel-in-progress: false  # Don't cancel training jobs
```

FEATURE VALIDATION:
```yaml
- name: Ensure features exist (fallback rebuild 365 days if missing)
  env:
    OPENWEATHER_API_KEY: ${{ secrets.OPENWEATHER_API_KEY }}
  run: |
    if [ ! -f Data/feature_store/karachi_daily_features.parquet ]; then 
      echo 'Features parquet missing. Rebuilding last 365 days...';
      python Data_Collection/feature_store_pipeline.py --days 365 --impute_short_gaps --min_hours_per_day 16;
    else
      echo 'Found features parquet.';
    fi
```

MODEL TRAINING PIPELINE:
```yaml
- name: Train LightGBM
  run: python Models/train_lightgbM.py --holdout_days 90 --tune --tune_trials 25

- name: Train HGBR
  run: python Models/train_hgbr.py --holdout_days 90 --tune --tune_trials 20

- name: Train Linear
  run: python Models/train_linear.py --holdout_days 90

- name: Train RandomForest
  run: python Models/train_rf.py --holdout_days 90

- name: Stack and calibrate
  run: python Models/stacking_linear_lightgbm.py --holdout_days 90 --constrain_nonneg --sum_to_one --calibrate
```

PREDICTION AND ALERTING:
```yaml
- name: Predict latest
  run: python Models/predict_latest.py

- name: Alert on hazardous AQI (hd1 >= 200)
  shell: bash
  run: |
    python - <<'PY'
    import json, sys
    with open('Models/registry/latest_forecast.json','r',encoding='utf-8') as f:
        blob = json.load(f)
    hd1 = (blob.get('blend') or {}).get('hd1')
    if hd1 is None:
        print('No hd1 value in latest forecast; skipping alert')
        sys.exit(0)
    try:
        v = float(hd1)
    except Exception:
        v = -1
    if v >= 200.0:
        print(f'HAZARDOUS AQI ALERT: blended hd1={v:.1f} >= 200')
        sys.exit(1)  # Fail job to raise visibility
    else:
        print(f'OK: blended hd1={v:.1f} < 200')
    PY
```

ARTIFACT MANAGEMENT:
```yaml
- name: Upload summaries and models
  uses: actions/upload-artifact@v4
  with:
    name: train-artifacts-${{ github.run_id }}
    path: |
      EDA/lightgbm_output/summary.json
      EDA/hgbr_output/summary.json
      EDA/linear_output/summary.json
      EDA/blend_output/summary.json
      Models/registry/*_preds.csv
      Models/registry/blend_weights_*.json
      Models/registry/latest_forecast.json
    retention-days: 14
```

============================================
6. DEPLOYMENT STRATEGIES
============================================

DEPLOYMENT OPTIONS:

1. **Local Development**:
```bash
# Run directly with Python
cd WebApp/Backend
uvicorn app.main:app --reload --port 8000

# Run with Docker
docker build -t aqi-predictor .
docker run -p 8000:8000 aqi-predictor
```

2. **Production Deployment**:
```bash
# Build production image
docker build -t aqi-predictor:prod .

# Run with production environment variables
docker run -d \
  --name aqi-predictor-prod \
  -p 8000:8000 \
  -e OPENWEATHER_API_KEY=$OPENWEATHER_API_KEY \
  -e FEAST_REPO_PATH=/app/feature_repo \
  -v /path/to/feature_repo:/app/feature_repo \
  -v /path/to/models:/app/Models/registry \
  aqi-predictor:prod
```

3. **Docker Compose**:
```yaml
# docker-compose.yml
version: '3.8'
services:
  aqi-predictor:
    build: .
    ports:
      - "8000:8000"
    environment:
      - OPENWEATHER_API_KEY=${OPENWEATHER_API_KEY}
      - FEAST_REPO_PATH=/app/feature_repo
    volumes:
      - ./feature_repo:/app/feature_repo
      - ./Models/registry:/app/Models/registry
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

4. **Kubernetes Deployment**:
```yaml
# k8s-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: aqi-predictor
spec:
  replicas: 3
  selector:
    matchLabels:
      app: aqi-predictor
  template:
    metadata:
      labels:
        app: aqi-predictor
    spec:
      containers:
      - name: aqi-predictor
        image: aqi-predictor:latest
        ports:
        - containerPort: 8000
        env:
        - name: OPENWEATHER_API_KEY
          valueFrom:
            secretKeyRef:
              name: aqi-secrets
              key: openweather-api-key
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5
---
apiVersion: v1
kind: Service
metadata:
  name: aqi-predictor-service
spec:
  selector:
    app: aqi-predictor
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8000
  type: LoadBalancer
```

============================================
7. MONITORING AND HEALTH CHECKS
============================================

HEALTH CHECK IMPLEMENTATION:

ENDPOINT HEALTH:
```python
@app.get("/health")
def health() -> Dict[str, Any]:
    latest_ts: Optional[str] = None
    try:
        row = feast_client.get_latest_offline_row(FEATURES_PARQUET)
        if row is not None and "event_timestamp" in row.index:
            latest_ts = str(row["event_timestamp"])
    except Exception:
        latest_ts = None

    artifacts = model_loader.current_artifacts_summary()
    return {
        "status": "ok",
        "latest_feature_timestamp": latest_ts,
        "artifacts": artifacts,
    }
```

HEALTH CHECK COMPONENTS:
1. **Feature Store Health**: Latest feature timestamp availability
2. **Model Registry Health**: Model artifacts status and availability
3. **API Responsiveness**: Endpoint response time and availability

MONITORING METRICS:
```python
# Example monitoring implementation
from prometheus_client import Counter, Histogram, generate_latest
import time

# Metrics
PREDICTION_COUNTER = Counter('aqi_predictions_total', 'Total AQI predictions made')
PREDICTION_DURATION = Histogram('aqi_prediction_duration_seconds', 'Time spent on predictions')

@app.get("/metrics")
def metrics():
    return generate_latest()

@app.get("/predict/latest")
def predict_latest() -> Dict[str, Any]:
    start_time = time.time()
    try:
        # ... prediction logic ...
        PREDICTION_COUNTER.inc()
        return pred
    finally:
        PREDICTION_DURATION.observe(time.time() - start_time)
```

============================================
8. ENVIRONMENT CONFIGURATION
============================================

ENVIRONMENT VARIABLES (.env):

The application uses a centralized `.env` file for configuration:

```bash
# Real-time update configuration
REALTIME_UPDATE_ENABLED=true
UPDATE_INTERVAL_MINUTES=30

# Server configuration
PORT=8000
HOST=0.0.0.0

# Path configuration
FEAST_REPO_PATH=feature_repo
FEATURES_PARQUET=Data/feature_store/karachi_daily_features.parquet
REGISTRY_DIR=Models/registry

# API keys (set these in production)
OPENWEATHER_API_KEY=your_api_key_here
```

CONFIGURATION MANAGEMENT:

1. **Development**: Uses `.env` file in project root
2. **Production**: Environment variables set in container/deployment
3. **Fallbacks**: Sensible defaults for all configuration values
4. **Security**: API keys never committed to version control

============================================
9. SECURITY CONSIDERATIONS
============================================

SECURITY IMPLEMENTATIONS:

1. **Environment Variables**:
```bash
# Never hardcode sensitive information
OPENWEATHER_API_KEY=your_api_key_here
FEAST_REPO_PATH=/app/feature_repo
MODEL_REGISTRY_PATH=/app/Models/registry
```

2. **CORS Configuration**:
```python
# Configure CORS appropriately for production
app.add_middleware(
    CORSMiddleware,
    allow_origins=["https://yourdomain.com"],  # Restrict origins
    allow_credentials=True,
    allow_methods=["GET", "POST"],
    allow_headers=["*"],
)
```

3. **Rate Limiting**:
```python
# Example rate limiting implementation
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded

limiter = Limiter(key_func=get_remote_address)
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)

@app.get("/predict/latest")
@limiter.limit("10/minute")  # 10 requests per minute per IP
def predict_latest(request: Request):
    # ... prediction logic ...
```

4. **Input Validation**:
```python
from pydantic import BaseModel, validator

class PredictionRequest(BaseModel):
    city_id: int
    forecast_days: int = 3
    
    @validator('forecast_days')
    def validate_forecast_days(cls, v):
        if v < 1 or v > 7:
            raise ValueError('forecast_days must be between 1 and 7')
        return v
```

============================================
9. PERFORMANCE OPTIMIZATION
============================================

OPTIMIZATION STRATEGIES:

1. **Model Caching**:
```python
# Cache models in memory for faster inference
import joblib
from functools import lru_cache

@lru_cache(maxsize=1)
def load_models():
    """Cache model loading to avoid repeated disk I/O"""
    models = {}
    for horizon in ['hd1', 'hd2', 'hd3']:
        model_path = f"Models/registry/hgb_{horizon}_latest.joblib"
        models[horizon] = joblib.load(model_path)
    return models
```

2. **Async Processing**:
```python
import asyncio
from concurrent.futures import ThreadPoolExecutor

@app.get("/predict/batch")
async def predict_batch(city_ids: List[int]):
    """Process multiple predictions concurrently"""
    executor = ThreadPoolExecutor(max_workers=4)
    
    async def predict_single(city_id):
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(executor, predict_for_city, city_id)
    
    tasks = [predict_single(city_id) for city_id in city_ids]
    results = await asyncio.gather(*tasks)
    return {"predictions": results}
```

3. **Response Caching**:
```python
from fastapi_cache import FastAPICache
from fastapi_cache.backends.redis import RedisBackend
from fastapi_cache.decorator import cache

@app.get("/predict/latest")
@cache(expire=300)  # Cache for 5 minutes
async def predict_latest():
    # ... prediction logic ...
```

============================================
10. TROUBLESHOOTING AND DEBUGGING
============================================

COMMON ISSUES AND SOLUTIONS:

1. **Model Loading Failures**:
```bash
# Check model registry
ls -la Models/registry/

# Verify model files exist
python -c "import joblib; print(joblib.load('Models/registry/hgb_hd1_latest.joblib'))"

# Check file permissions
chmod 644 Models/registry/*.joblib
```

2. **Feature Store Issues**:
```bash
# Check Feast configuration
cd feature_repo
feast config

# Verify online store
feast get-online-features \
  --feature-service karachi_air_quality_daily \
  --entity '{"karachi_id": 1}'
```

3. **Docker Issues**:
```bash
# Check container logs
docker logs aqi-predictor

# Inspect container
docker exec -it aqi-predictor bash

# Check resource usage
docker stats aqi-predictor
```

4. **GitHub Actions Failures**:
```bash
# Check workflow runs
gh run list --workflow=features-hourly.yml

# View specific run logs
gh run view --log <run_id>

# Re-run failed workflow
gh run rerun <run_id>
```

============================================
11. FUTURE ENHANCEMENTS
============================================

PLANNED IMPROVEMENTS:

1. **Short Term (1-3 months)**:
   - Implement comprehensive logging with structured logging
   - Add metrics collection with Prometheus
   - Implement automated testing pipeline
   - Add API versioning support

2. **Medium Term (3-6 months)**:
   - Implement horizontal scaling with load balancers
   - Add database persistence for prediction history
   - Implement user authentication and authorization
   - Add real-time WebSocket support for live updates

3. **Long Term (6+ months)**:
   - Implement microservices architecture
   - Add support for multiple cities and regions
   - Implement advanced caching strategies
   - Add machine learning model versioning and A/B testing

============================================
14. CRITICAL WEBAPP COMPONENTS
============================================

FEATURE ALIGNMENT AND COERCION:
```python
# From model_loader.py
def _align_to_feature_names(df, feature_names):
    """Return a 1-row DataFrame with exactly feature_names columns in order"""
    row = {}
    for name in feature_names:
        val = df[name].iloc[0] if name in df.columns else np.nan
        row[name] = pd.to_numeric(val, errors="coerce")
    X = pd.DataFrame([row], columns=feature_names)
    X = X.replace([np.inf, -np.inf], np.nan).fillna(0.0)
    return X

def _coerce_numeric_impute_latest(df, feature_names):
    """Coerce features to numeric and handle missing values"""
    row = {}
    for name in feature_names:
        val = df[name].iloc[0] if name in df.columns else np.nan
        row[name] = pd.to_numeric(val, errors="coerce")
    X = pd.DataFrame([row], columns=feature_names)
    X = X.replace([np.inf, -np.inf], np.nan).fillna(0.0)
    return X
```

MODEL LOADING FALLBACK MECHANISMS:
```python
# From model_loader.py
def initialize_registry(registry_dir: str) -> None:
    """Initialize model registry with fallback handling"""
    global _REGISTRY_DIR, _BLEND_WEIGHTS
    _REGISTRY_DIR = registry_dir

    # Load LightGBM boosters (optional)
    if _lgb_runtime is not None:
        for hz in ["hd1", "hd2", "hd3"]:
            path = _latest(os.path.join(_REGISTRY_DIR, f"lgb_{hz}_*.txt"))
            if path:
                try:
                    _BOOSTERS[hz] = _lgb_runtime.Booster(model_file=path)
                except Exception:
                    _BOOSTERS[hz] = None

    # Load linear pipelines with feature lists
    if joblib is not None:
        for hz in ["hd1", "hd2", "hd3"]:
            path = _latest(os.path.join(_REGISTRY_DIR, f"linear_{hz}_*.joblib"))
            if path:
                try:
                    payload = joblib.load(path)
                    _LINEAR[hz] = payload.get("model")
                    _LINEAR_FEATS[hz] = payload.get("features", [])
                except Exception as e:
                    _LINEAR[hz] = None
                    _LINEAR_FEATS[hz] = []
```

DEBUG LOGGING IMPLEMENTATION:
```python
# From model_loader.py
def predict_all_from_series(latest: pd.Series) -> Dict[str, Dict[str, float]]:
    df = _to_frame(latest)
    out: Dict[str, Dict[str, float]] = {
        "latest_feature_timestamp": str(latest.get("event_timestamp", "")),
        "lightgbm": {},
        "linear": {},
        "hgbr": {},
        "blend": {},
    }

    # DEBUG: Print what models are loaded
    print(f"DEBUG: _BOOSTERS: {_BOOSTERS}")
    print(f"DEBUG: _LINEAR: {_LINEAR}")
    print(f"DEBUG: _HGBR: {_HGBR}")
    print(f"DEBUG: _LINEAR_FEATS: {_LINEAR_FEATS}")
    print(f"DEBUG: _HGBR_FEATS: {_HGBR_FEATS}")

    # Per-model predictions with debug logging
    for hz in ["hd1", "hd2", "hd3"]:
        print(f"DEBUG: Processing horizon {hz}")
        # ... prediction logic with debug prints ...
    
    print(f"DEBUG: Final predictions: {out}")
    return out
```

============================================
12. REAL-TIME PREDICTION SYSTEM
============================================

REAL-TIME PREDICTION ARCHITECTURE:

The system now implements a comprehensive real-time prediction system that automatically updates data and models:

```python
# Models/predict_realtime.py - Core real-time prediction logic
def main():
    """Main function for real-time prediction generation"""
    try:
        # Validate feature freshness
        features_path = project_root / "Data" / "feature_store" / "karachi_daily_features.parquet"
        if not validate_feature_freshness(str(features_path)):
            logger.error("Features are not current. Please update features first.")
            return False
        
        # Load models and generate predictions
        predictions = generate_realtime_predictions()
        
        # Save predictions with timestamp
        save_predictions(predictions)
        
        logger.info("Real-time predictions generated successfully")
        return True
        
    except Exception as e:
        logger.error(f"Failed to generate real-time predictions: {e}")
        return False

def validate_feature_freshness(features_path: str) -> bool:
    """Validate that features are current (have today's data)"""
    try:
        if not os.path.exists(features_path):
            return False
        
        df = pd.read_parquet(features_path)
        if df.empty:
            return False
        
        latest_ts = pd.to_datetime(df['event_timestamp'].max())
        current_ts = datetime.now(timezone.utc)
        
        # Check if we have today's data (not just hours old)
        latest_date = latest_ts.date()
        current_date = current_ts.date()
        
        logger.info(f"Latest feature date: {latest_date}")
        logger.info(f"Current date: {current_date}")
        
        if latest_date >= current_date:
            logger.info("✅ Features are current - have today's data")
            return True
        else:
            logger.warning(f"Features are outdated - last: {latest_date}, current: {current_date}")
            return False
            
    except Exception as e:
        logger.error(f"Error validating feature freshness: {e}")
        return False
```

AUTOMATED UPDATE WORKFLOW:

1. **Background Worker**: Runs every 30 minutes automatically
2. **Smart Detection**: Only updates when data is actually outdated
3. **Gap Filling**: Automatically detects and fills missing date ranges
4. **Model Retraining**: Updates models with latest data
5. **Prediction Generation**: Creates fresh predictions for upcoming days
6. **UI Refresh**: Frontend automatically reflects latest predictions

REAL-TIME FEATURES:

- **Dynamic Date Calculation**: Predictions always show actual upcoming dates
- **Live Data Updates**: Continuous data collection from weather APIs
- **Smart Caching**: Only fetches new data when needed
- **Background Processing**: Updates don't block user interactions
- **Manual Triggers**: Users can force updates via UI button
- **Status Monitoring**: Real-time update status and next scheduled update

============================================
13. DOCKER TESTING AND VALIDATION
============================================

DOCKER CONFIGURATION TESTING:

**1. Build Validation**:
```bash
# Test Docker build
docker build -t aqi-predictor:test .

# Verify build success
docker images | grep aqi-predictor
```

**2. Container Structure Validation**:
```bash
# Run container in interactive mode to inspect structure
docker run -it --rm aqi-predictor:test bash

# Inside container, verify file structure
ls -la /app/
ls -la /app/WebApp/
ls -la /app/WebApp/Backend/app/
ls -la /app/WebApp/Frontend/

# Verify Python imports work
python -c "from WebApp.Backend.app.main import app; print('FastAPI app loaded successfully')"
python -c "from WebApp.Frontend.gradio_app import demo; print('Gradio app loaded successfully')"
```

**3. Health Check Validation**:
```bash
# Run container with health checks
docker run -d --name aqi-test \
  -p 8000:8000 \
  aqi-predictor:test

# Wait for health check
sleep 30

# Check health status
docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}"

# Test health endpoint
curl -f http://localhost:8000/health

# Check container logs
docker logs aqi-test

# Cleanup
docker stop aqi-test
docker rm aqi-test
```

**4. Full Application Testing**:
```bash
# Run container with volume mounts
docker run -d --name aqi-full-test \
  -p 8000:8000 \
  -v $(pwd)/feature_repo:/app/feature_repo \
  -v $(pwd)/Models/registry:/app/Models/registry \
  -v $(pwd)/Data/feature_store:/app/Data/feature_store \
  aqi-predictor:test

# Wait for startup
sleep 30

# Test API endpoints
curl http://localhost:8000/health
curl http://localhost:8000/predict/latest

# Test Gradio interface
curl http://localhost:8000/ui/

# Check container logs for errors
docker logs aqi-full-test

# Cleanup
docker stop aqi-full-test
docker rm aqi-full-test
```

**5. Production Readiness Checklist**:
```bash
# Security validation
docker run --rm -v /var/run/docker.sock:/var/run/docker.sock \
  aquasec/trivy image aqi-predictor:test

# Resource usage testing
docker run -d --name aqi-stress-test \
  -p 8000:8000 \
  --memory=512m \
  --cpus=1.0 \
  aqi-predictor:test

# Load testing
ab -n 1000 -c 10 http://localhost:8000/health

# Cleanup
docker stop aqi-stress-test
docker rm aqi-stress-test
```

**6. Troubleshooting Common Docker Issues**:

**Issue: Import Errors**:
```bash
# Check if WebApp structure is correct in container
docker run -it --rm aqi-predictor:test bash
ls -la /app/WebApp/Backend/app/
python -c "import sys; print(sys.path)"
```

**Issue: File Permission Errors**:
```bash
# Verify user permissions
docker run -it --rm aqi-predictor:test bash
whoami
ls -la /app/
```

**Issue: Health Check Failures**:
```bash
# Check if curl is available
docker run -it --rm aqi-predictor:test bash
which curl
curl --version
```

**Issue: Port Binding Problems**:
```bash
# Check if port is already in use
netstat -tulpn | grep :8000

# Use different port
docker run -p 8001:8000 aqi-predictor:test
```

============================================
15. CURRENT SYSTEM SUMMARY
============================================

SYSTEM OVERVIEW:

The Karachi AQI Prediction System is now a fully automated, real-time prediction engine:

**Architecture**:
- **Backend**: FastAPI with integrated real-time update system
- **Frontend**: Gradio interface with live data visualization
- **Data Pipeline**: Automated feature collection and model retraining
- **Deployment**: Docker containerized with health checks and monitoring

**Real-Time Capabilities**:
- **Automated Updates**: Every 30 minutes, system checks for new data
- **Smart Detection**: Only updates when data is actually outdated
- **Gap Filling**: Automatically detects and fills missing date ranges
- **Live Predictions**: Always shows predictions for actual upcoming dates
- **Background Processing**: Updates don't block user interactions

**User Experience**:
- **Dynamic UI**: Real-time prediction boxes with color-coded AQI levels
- **Interactive Charts**: 30-day historical data with live forecast overlay
- **Model Selection**: Dropdown to choose between different ML models
- **Manual Control**: Force update button for immediate data refresh
- **Status Monitoring**: Real-time update status and next scheduled update

**Technical Features**:
- **Health Checks**: Docker health monitoring and API health endpoints
- **Error Handling**: Comprehensive error handling with user-friendly messages
- **Logging**: Detailed logging for debugging and monitoring
- **Configuration**: Centralized environment variable management
- **Security**: Non-root Docker user and proper file permissions

**Deployment**:
- **Containerization**: Optimized Docker image with multi-stage builds
- **CI/CD**: Automated GitHub Actions for feature building and model training
- **Monitoring**: Health check endpoints and real-time status monitoring
- **Scalability**: Async FastAPI backend with background worker threads

This system represents a production-ready, enterprise-grade AQI prediction platform that continuously learns and adapts to provide the most accurate and up-to-date air quality forecasts.

============================================
END OF DOCUMENTATION
============================================
